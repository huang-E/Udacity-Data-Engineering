{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, split,unix_timestamp, to_date\n",
    "import os, time\n",
    "from  pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "-   The project will use the data and template provided by Udacity. The data will be mainly I94 immigration data, U.S. City Demographic Data and Airport Code data.The purpose of the project is to create a data model based on the provided data set, so we can quickly query, slicing and dicing the data using the proposed data model.After the ETL process the fact table will also be saved and partitioned in Parquet format for fast read. Main tool for this project will be Pyspark.\n",
    "\n",
    "\n",
    "\n",
    "#### Describe and Gather Data\n",
    "-   **I94 Immigration Data:**  This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace.  [This](https://travel.trade.gov/research/reports/i94/historical/2016.html)  is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project. \n",
    "    \n",
    "\n",
    "   \n",
    "  \n",
    "-   **U.S. City Demographic Data:**  This data comes from OpenSoft. You can read more about it  [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "    Columns details\n",
    "    \n",
    "-   **Airport Code Table:**  This is a simple table of airport codes and corresponding cities. It comes from  [here](https://datahub.io/core/airport-codes#data).\n",
    "    Columns details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "checking duplicate data (No duplicate data found in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check duplicates for airpot data\n",
    "fn = 'airport-codes_csv.csv'\n",
    "df_airport_check = spark.read.format('csv').options(header='true', inferSchema='true').load(fn)\n",
    "\n",
    "if df_airport_check.count() > df_airport_check.dropDuplicates(df_airport_check.columns).count():\n",
    "    raise ValueError('Data has duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check duplicates for demographic data\n",
    "fn_demographics = 'us-cities-demographics.csv'\n",
    "df_us_demographics_check= spark.read.csv(fn_demographics, header='true', sep=\";\")\n",
    "\n",
    "\n",
    "if df_us_demographics_check.count() > df_us_demographics_check.dropDuplicates(df_us_demographics_check.columns).count():\n",
    "    raise ValueError('Data has duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check duplicates for immi data\n",
    "df_spark_immi_check =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "if df_spark_immi_check.count() > df_spark_immi_check.dropDuplicates(df_spark_immi_check.columns).count():\n",
    "    raise ValueError('Data has duplicates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "missing values check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|  0.0| 0.0| 0.0|      7006.0|      0.0|        0.0|       0.0|      5676.0| 14045.0|  45886.0|   26389.0|        0.0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking missing value in airport data\n",
    "\n",
    "#https://stackoverflow.com/questions/44413132/count-the-number-of-missing-values-in-a-dataframe-spark\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "rows = df_airport_check.count()\n",
    "summary = df_airport_check.describe().filter(col(\"summary\") == \"count\")\n",
    "summary.select(*((lit(rows)-col(c)).alias(c) for c in df_airport_check.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "| 0.0|  0.0|       0.0|            3.0|              3.0|             0.0|              13.0|        13.0|                  16.0|       0.0| 0.0|  0.0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking missing value in demographic data\n",
    "rows = df_us_demographics_check.count()\n",
    "summary = df_us_demographics_check.describe().filter(col(\"summary\") == \"count\")\n",
    "summary.select(*((lit(rows)-col(c)).alias(c) for c in df_us_demographics_check.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+--------+--------+------+-------+-----+--------+---------+---------+-------+--------+---------+--------+-------+-------+--------+---------+-------+------+-------+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode| i94addr| depdate|i94bir|i94visa|count|dtadfile| visapost|    occup|entdepa| entdepd|  entdepu| matflag|biryear|dtaddto|  gender|   insnum|airline|admnum|  fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+--------+--------+------+-------+-----+--------+---------+---------+-------+--------+---------+--------+-------+-------+--------+---------+-------+------+-------+--------+\n",
      "|  0.0|  0.0|   0.0|   0.0|   0.0|    0.0|    0.0|  239.0|152592.0|142457.0| 802.0|    0.0|  0.0|     1.0|1881250.0|3088187.0|  238.0|138429.0|3095921.0|138429.0|  802.0|  477.0|414269.0|2982605.0|83627.0|   0.0|19549.0|     0.0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+--------+--------+------+-------+-----+--------+---------+---------+-------+--------+---------+--------+-------+-------+--------+---------+-------+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking missing value in immi data\n",
    "# no missing value found  i94yr, i94mon, i94cit, i94res, i94port, aardate, i94visa, these columns can be used as forein key to join the dim tables\n",
    "\n",
    "rows = df_spark_immi_check.count()\n",
    "summary = df_spark_immi_check.describe().filter(col(\"summary\") == \"count\")\n",
    "summary.select(*((lit(rows)-col(c)).alias(c) for c in df_spark_immi_check.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### data extracted from I94_SAS_Labels_Descriptions.SAS saved in label_mapping for dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get us_state code dimension and transformed to spark dataframe\n",
    "fn = '/home/workspace/label_mapping/i94addrl.txt'\n",
    "df_us_state = pd.read_csv(fn , sep=\"=\", header=None, engine='python',  names = [\"state_code\", \"state\"], skipinitialspace = False)\n",
    "df_us_state['state_code'] = df_us_state['state_code'].str.replace('[^a-zA-Z]', '')\n",
    "df_us_state['state'] = df_us_state['state'].str.replace('[^a-zA-Z]', '')\n",
    "df_us_state_cleaned = df_us_state\n",
    "df_us_state_spark = spark.createDataFrame(df_us_state_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get country code and transformed\n",
    "fn ='/home/workspace/label_mapping/i94cntyl.txt'\n",
    "df_country_code = pd.read_csv(fn , sep=\"=\", header=None, engine='python',  names = [\"country_code\", \"country\"], skipinitialspace = False)\n",
    "df_country_code['country'] = df_country_code['country'].str.strip(\"\\' \\n\\t\")\n",
    "# replace start with INVALID, Collapsed, No Country Code with other\n",
    "pattern = '|'.join(['INVALID.*', 'Collapsed.*', 'No Country Code.*'])\n",
    "df_country_code['country']= df_country_code['country'].str.replace(pattern, 'others', regex = True)\n",
    "df_country_code_cleaned = df_country_code\n",
    "df_country_code_spark = spark.createDataFrame(df_country_code_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get arrival mode dimension and transformed\n",
    "fn = '/home/workspace/label_mapping/i94model.txt'\n",
    "df_mode = pd.read_csv(fn , sep=\"=\", header=None, engine='python',  names = [\"arrival_code\", \"arrival_mode\"], skipinitialspace = False)\n",
    "df_mode[\"arrival_mode\"]= df_mode[\"arrival_mode\"].str.strip(\"\\' \\n\\t\")\n",
    "df_mode_cleaned = df_mode\n",
    "df_mode_spark = spark.createDataFrame(df_mode_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get port info, after processing the data contains non US ports info, but we can use the 2 digit state code for US state\n",
    "fn = '/home/workspace/label_mapping/i94prtl.txt'\n",
    "df_port = pd.read_csv(fn , sep=\"=\", header=None, engine='python',  names = [\"port_code\", \"port_name\"], skipinitialspace = False)\n",
    "df_port[\"port_code\"]= df_port[\"port_code\"].str.strip(\"\\' \\n\\t\")\n",
    "df_port[\"port_name\"]= df_port[\"port_name\"].str.strip(\"\\' \\n\\t\")\n",
    "#only with records with comma sperator in port_name column\n",
    "df_port_valid=df_port[df_port[\"port_name\"].str.contains(\",\",regex=True)==True]\n",
    "#split port_name as port_name to get port_name and state name\n",
    "tmp= df_port_valid[\"port_name\"].str.rsplit(\",\", n=1, expand=True)\n",
    "#update the state code\n",
    "df_port_valid['state']=tmp[1].str.strip()\n",
    "#update the port name\n",
    "df_port_valid[\"port_name\"]=tmp[0].str.strip()\n",
    "df_port_cleaned = df_port_valid\n",
    "df_port_spark = spark.createDataFrame(df_port_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_visa get visa dimension and transform data\n",
    "fn = '/home/workspace/label_mapping/I94VISA.txt' \n",
    "df_visa = pd.read_csv(fn , sep=\"=\", header=None, engine='python',  names = [\"visa_code\", \"visa_type\"], skipinitialspace = False)\n",
    "df_visa_cleaned = df_visa\n",
    "df_visa_spark =  spark.createDataFrame(df_visa_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it took 0.08245205879211426 to run\n"
     ]
    }
   ],
   "source": [
    "# Get immi info prepare for fact table and transform data\n",
    "# reading april 16 IMMI data in SAS format\n",
    "start_time = time.time()\n",
    "df_spark_immi =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "# change arrdate to date format\n",
    "format_date = udf(lambda x: (datetime.datetime(1960, 1, 1).date() + datetime.timedelta(x)).isoformat() if x else None)\n",
    "df_spark_immi = df_spark_immi.withColumn(\"arrdate\", format_date(df_spark_immi.arrdate))\n",
    "df_spark_immi=df_spark_immi.withColumn('arrdate_in_dateFormat',to_date(unix_timestamp(col('arrdate'), 'yyyy-MM-dd').cast(\"timestamp\")))\n",
    "print(\"it took\", time.time() - start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read us-cities-demographics and transforming\n",
    "demographicsSchema = StructType([\n",
    "                            StructField(\"city\",StringType()),\n",
    "                            StructField(\"state\",StringType()),\n",
    "                            StructField(\"median_age\",DoubleType()),\n",
    "                            StructField(\"male_population\",StringType()),\n",
    "                            StructField(\"female_population\",StringType()),\n",
    "                            StructField(\"total_population\",IntegerType()),\n",
    "                            StructField(\"number_of_veterans\",IntegerType()),\n",
    "                            StructField(\"number_of_foreign_born\",IntegerType()),\n",
    "                            StructField(\"average_household_size\",DoubleType()),\n",
    "                            StructField(\"state_code\",StringType()),\n",
    "                            StructField(\"race\",StringType()),\n",
    "                            StructField(\"count\",IntegerType()) \n",
    "                            ])\n",
    "\n",
    "fn = 'us-cities-demographics.csv'\n",
    "df_us_demographics_spark= spark.read.csv(fn, header='true', sep=\";\", schema=demographicsSchema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read airport-codes file and transform\n",
    "fn = 'airport-codes_csv.csv'\n",
    "df_airport = spark.read.format('csv').options(header='true', inferSchema='true').load(fn)\n",
    "# change keep us airport info only, transform corodiates to latitutued and longittued iso\n",
    "\n",
    "df_airport_spark = df_airport.filter(df_airport['iso_country'] == 'US')\\\n",
    "          .withColumn('state', split(df_airport['iso_region'], \"-\").getItem(1))\\\n",
    "          .withColumn('latitude', split(df_airport['coordinates'], \" \").getItem(0).cast(DoubleType()))\\\n",
    "          .withColumn('longitude', split(df_airport['coordinates'], \" \").getItem(1).cast(DoubleType()))\\\n",
    "          .drop('iso_region')\\\n",
    "          .drop('coordinates')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "* Fact table will be based on I94 Immigration Data because it contains the travellor records\n",
    "* Dim table will be based on I94 Immigration data i94yr, i94mon, i94cit, i94res, i94port, i94visa,aardate(this will be used to partition the data when saved in parquet format)\n",
    "* Dim table will also be based on us-cities-demographics and airport-codes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Proposed data model will be shown as below\n",
    "\n",
    "<a href=\"https://ibb.co/twGtZnH\"><img src=\"https://i.ibb.co/4wqGsCF/datamodel.jpg\" alt=\"datamodel\" border=\"0\"></a><br /><a target='_blank' href='https://imgbb.com/'>image uploader</a><br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "\n",
    "* Prepare txt data based on I94_SAS_Labels_Descriptions.SAS saved in label_mapping folder, create dim tables based on them\n",
    "* Create Dim_us_demographcis table based on us-cities-demographics.csv \n",
    "* Create Dim_airport table based on airport-codes_csv.csv\n",
    "* Create Fact_immi table based on ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat data and dim_mode,dim_us_state and dim_mod\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using Pyspark sql to create data model\n",
    "df_us_state_spark.createOrReplaceTempView('df_us_state_spark')\n",
    "df_country_code_spark.createOrReplaceTempView('df_country_code_spark')\n",
    "df_mode_spark.createOrReplaceTempView('df_mode_spark')\n",
    "df_spark_immi.createOrReplaceTempView('df_spark_immi')\n",
    "df_port_spark.createOrReplaceTempView('df_port_spark')\n",
    "df_us_demographics_spark.createOrReplaceTempView('df_us_demographics_spark')\n",
    "df_airport_spark.createOrReplaceTempView('df_airport_spark')\n",
    "df_visa_spark.createOrReplaceTempView('df_visa_spark')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### create temp table \n",
    "df_spark_immi_temp = spark.sql(\"\"\"\n",
    "                                        select \n",
    "                                                i.cicid,\n",
    "                                                i.arrdate_in_dateFormat as arrival_date,\n",
    "                                                i.i94yr as year,\n",
    "                                                i.i94mon as month,\n",
    "                                                i.i94cit as birth_country,\n",
    "                                                i.i94res as residence_country,\n",
    "                                                i.i94bir as repondent_age,\n",
    "                                                i.biryear as birth_year,\n",
    "                                                i.i94addr as state_code,\n",
    "                                                i.i94mode as arrival_code,\n",
    "                                                i.i94port as port,\n",
    "                                                i.i94visa as visa_code,\n",
    "                                                i.visatype as visa_type\n",
    "                                            from df_spark_immi i \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create dim tables\n",
    "Dim_us_state = spark.sql(\"\"\"SELECT * FROM df_us_state_spark\"\"\")\n",
    "Dim_country_code = spark.sql(\"\"\"SELECT * FROM df_country_code_spark \"\"\")\n",
    "Dim_mode = spark.sql(\"\"\"SELECT * FROM  df_mode_spark\"\"\")\n",
    "Dim_port = spark.sql(\"\"\"SELECT * FROM df_port_spark\"\"\")\n",
    "Dim_us_demographics =(\"\"\"SELECT * FROM df_us_demographics_spark \"\"\")\n",
    "Dim_airport = (\"\"\"SELECT * FROM df_airport_spark\"\"\")\n",
    "Dim_visa_code =(\"\"\"SELECT * FROM df_visa_spark\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create fact table\n",
    "df_spark_immi_temp.createOrReplaceTempView('df_spark_immi_temp')\n",
    "dim_us_state.createOrReplaceTempView('dim_us_state')\n",
    "dim_mode.createOrReplaceTempView('dim_mode')\n",
    "\n",
    "fact_immi = spark.sql(\"\"\"SELECT                   t.cicid,\n",
    "                                                t.arrival_date,\n",
    "                                                t.year,\n",
    "                                                t.month,\n",
    "                                                t.birth_country,\n",
    "                                                t.residence_country,\n",
    "                                                t.repondent_age,\n",
    "                                                t.birth_year,\n",
    "                                                t.state_code,\n",
    "                                                t.arrival_code,\n",
    "                                                t.port,\n",
    "                                                t.visa_code,\n",
    "                                                t.visa_type,\n",
    "                                                coalesce(m.arrival_mode, 'Not reported') as arrival_mode,\n",
    "                                                coalesce(s.state_code, '99') as state\n",
    "                                               \n",
    "                                                \n",
    "                                              \n",
    "\n",
    "FROM df_spark_immi_temp t\n",
    "left join dim_us_state S on t.state_code=s.state_code\n",
    "left join dim_mode m on t.arrival_code = m.arrival_code\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Save result with partition as parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write result in  parquet formt to sas_data folder\n",
    "fact_immi.write.mode(\"overwrite\").parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write data to parquet and partition by year and and month and state\n",
    "fact_immi.write.mode(\"overwrite\").partitionBy(\"year\", \"month\", \"state\").parquet(\"partitioned_result\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check the immi data records are not missing after fact table created\n",
    "df_spark_immi_then =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark_immi_now = spark.read.parquet(\"sas_data\")\n",
    "assert(df_spark_immi_then.count() == df_spark_immi_now.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+-----+-------------+-----------------+-------------+----------+----------+------------+----+---------+---------+------------+-----+---------+--------+\n",
      "|    cicid|arrival_date|  year|month|birth_country|residence_country|repondent_age|birth_year|state_code|arrival_code|port|visa_code|visa_type|arrival_mode|state|visa_code| country|\n",
      "+---------+------------+------+-----+-------------+-----------------+-------------+----------+----------+------------+----+---------+---------+------------+-----+---------+--------+\n",
      "| 881270.0|  2016-04-05|2016.0|  4.0|        299.0|            299.0|         34.0|    1982.0|        AZ|         1.0| SFR|      1.0|       B1|         Air|   AZ|        1|MONGOLIA|\n",
      "|1048471.0|  2016-04-06|2016.0|  4.0|        299.0|            299.0|         51.0|    1965.0|        AZ|         1.0| SFR|      1.0|       B1|         Air|   AZ|        1|MONGOLIA|\n",
      "|1048473.0|  2016-04-06|2016.0|  4.0|        299.0|            299.0|         45.0|    1971.0|        AZ|         1.0| SFR|      1.0|       B1|         Air|   AZ|        1|MONGOLIA|\n",
      "+---------+------------+------+-----+-------------+-----------------+-------------+----------+----------+------------+----+---------+---------+------------+-----+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data quality check to validate the fact and dim tables join working, country name successfully loaded\n",
    "fact_immi.createOrReplaceTempView('fact_immi')\n",
    "Dim_country_code.createOrReplaceTempView('Dim_country_code')\n",
    "result_country =spark.sql(\"\"\"\n",
    "select i.*, c.country from fact_immi i left join Dim_country_code c\n",
    "on i.birth_country = c.country_code\"\"\")\n",
    "result_country.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Data quality check for fact table to ensure arrival_date is correct\n",
    "assert(spark.sql(\"select arrival_date from fact_immi where dayofweek(arrival_date)<1 or dayofweek(arrival_date)>7\").count() == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Data dictionary for data model has been saved as datadiconary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "  <em>Pyspark is chosen because the for one month of data it is about 6GB. Apache Spark is a popular open source framework that ensures data processing with lightning speed and supports various   languages like Python </em>\n",
    "\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "  <em>Data should be updated at least daily if not hourly, because immi visitor data are part of the critical inforamtion for country's border secutiry management</em>\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    " <em>When data was increased by 100x big data solution is required cloud solution, GCP cloud storage, AWS S3 bucket could be the good candiates. </em>\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    " <em>This can be achieved through combintion of Apache airflow and AWS quicksight or GCP's data studio or third party visualisation software </em>\n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    " <em>AWS redshift or GCP big Query can provide 100+ people access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6.0G\n",
      "-rw-r--r-- 1 root root 451M May 31  2018 i94_apr16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 597M May 31  2018 i94_aug16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 500M May 31  2018 i94_dec16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 374M May 31  2018 i94_feb16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 415M May 31  2018 i94_jan16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 620M May 31  2018 i94_jul16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 684M May 31  2018 i94_jun16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 459M May 31  2018 i94_mar16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 501M May 31  2018 i94_may16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 424M May 31  2018 i94_nov16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 531M May 31  2018 i94_oct16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 543M May 31  2018 i94_sep16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "%ls -lh ../../data/18-83510-I94-Data-2016/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
